{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "BERT Word Embeddings Deep Dive.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcm5Iri7qE2q"
   },
   "source": [
    "# **Installing the necessary libararies**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N7iUWdmx8FNu",
    "outputId": "9285377d-f224-417d-aeab-84246a2cf895",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install sklearn\n",
    "!pip install pytorch_transformers\n",
    "!pip install transformers\n",
    "!pip install matplotlib\n"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (1.21.5)\r\n",
      "Requirement already satisfied: torch in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (1.10.2)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from torch) (4.0.1)\r\n",
      "Requirement already satisfied: sklearn in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sklearn) (1.0.2)\r\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.21.5)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from scikit-learn->sklearn) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.1.0)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.7.3)\r\n",
      "Requirement already satisfied: pytorch_transformers in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (1.2.0)\r\n",
      "Requirement already satisfied: boto3 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (1.20.49)\r\n",
      "Requirement already satisfied: requests in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (2.27.1)\r\n",
      "Requirement already satisfied: numpy in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (1.21.5)\r\n",
      "Requirement already satisfied: sentencepiece in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (0.1.96)\r\n",
      "Requirement already satisfied: regex in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (2022.1.18)\r\n",
      "Requirement already satisfied: tqdm in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (4.62.3)\r\n",
      "Requirement already satisfied: torch>=1.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (1.10.2)\r\n",
      "Requirement already satisfied: sacremoses in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (0.0.47)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from torch>=1.0.0->pytorch_transformers) (4.0.1)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from boto3->pytorch_transformers) (0.10.0)\r\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.49 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from boto3->pytorch_transformers) (1.23.49)\r\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from boto3->pytorch_transformers) (0.5.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.49->boto3->pytorch_transformers) (1.26.8)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.49->boto3->pytorch_transformers) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.49->boto3->pytorch_transformers) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->pytorch_transformers) (2.0.11)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->pytorch_transformers) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->pytorch_transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: joblib in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->pytorch_transformers) (1.1.0)\r\n",
      "Requirement already satisfied: click in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->pytorch_transformers) (8.0.3)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from click->sacremoses->pytorch_transformers) (4.10.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->pytorch_transformers) (3.7.0)\r\n",
      "Requirement already satisfied: transformers in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (4.16.2)\r\n",
      "Requirement already satisfied: requests in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (2.27.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (1.21.5)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (4.10.1)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (0.4.0)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (4.62.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (2022.1.18)\r\n",
      "Requirement already satisfied: sacremoses in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (0.0.47)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (0.11.4)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: filelock in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (3.4.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.7)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.0)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->transformers) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->transformers) (2.0.11)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: joblib in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\r\n",
      "Requirement already satisfied: six in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "Requirement already satisfied: click in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\r\n",
      "Requirement already satisfied: matplotlib in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (3.5.1)\r\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from matplotlib) (9.0.1)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from matplotlib) (1.3.2)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from matplotlib) (1.21.5)\r\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from matplotlib) (3.0.7)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from matplotlib) (4.29.1)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from matplotlib) (21.3)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZovDAtnWQsg"
   },
   "source": [
    "# **Loading the Pre-trained BERT model**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xKtXV_rd8cwa",
    "outputId": "a26426be-429b-4b2f-99c3-7263b372ba63",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "## Load pretrained model/tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/Users/lfoppiano/development/projects/embeddings/pre-trained-embeddings/matscibert\")\n",
    "model = AutoModel.from_pretrained(\"/Users/lfoppiano/development/projects/embeddings/pre-trained-embeddings/matscibert\",output_hidden_states=True)\n"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/lfoppiano/development/projects/embeddings/pre-trained-embeddings/matscibert were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at /Users/lfoppiano/development/projects/embeddings/pre-trained-embeddings/matscibert and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGbXXZ9_WfFX"
   },
   "source": [
    "# **Input Formatting (Tokenization)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XzUlQMCA8RpQ",
    "outputId": "820a23f7-779d-42aa-cb23-7917fbf49ca4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    }
   },
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text1 = \"We are studying the material La3A2Ge2 (A = Ir, Rh).\"\n",
    "text2 = \"The critical temperature T C = 4.7 K discovered for La3Ir2Ge2 in this work is by about 1.2 K higher than that found for La3Rh2Ge2.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text1 + \" [SEP] \" + text2 + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for idx, tup in enumerate(zip(tokenized_text, indexed_tokens)):\n",
    "    print('{:<3} {:<12} {:>6,}'.format(idx, tup[0], tup[1]))"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   [CLS]           102\n",
      "1   we              185\n",
      "2   are             220\n",
      "3   studying      8,467\n",
      "4   the             111\n",
      "5   material      1,440\n",
      "6   la            1,665\n",
      "7   ##3          30,138\n",
      "8   ##a          30,110\n",
      "9   ##2          30,132\n",
      "10  ##ge            303\n",
      "11  ##2          30,132\n",
      "12  (               145\n",
      "13  a               106\n",
      "14  =               275\n",
      "15  ir            1,622\n",
      "16  ,               422\n",
      "17  rh            3,645\n",
      "18  )               546\n",
      "19  .               205\n",
      "20  [SEP]           103\n",
      "21  the             111\n",
      "22  critical      2,616\n",
      "23  temperature   1,633\n",
      "24  t               105\n",
      "25  c               115\n",
      "26  =               275\n",
      "27  4               286\n",
      "28  .               205\n",
      "29  7               450\n",
      "30  k               231\n",
      "31  discovered    8,847\n",
      "32  for             168\n",
      "33  la            1,665\n",
      "34  ##3          30,138\n",
      "35  ##ir            211\n",
      "36  ##2          30,132\n",
      "37  ##ge            303\n",
      "38  ##2          30,132\n",
      "39  in              121\n",
      "40  this            238\n",
      "41  work            697\n",
      "42  is              165\n",
      "43  by              214\n",
      "44  about         1,011\n",
      "45  1               158\n",
      "46  .               205\n",
      "47  2               170\n",
      "48  k               231\n",
      "49  higher        1,001\n",
      "50  than            506\n",
      "51  that            198\n",
      "52  found           797\n",
      "53  for             168\n",
      "54  la            1,665\n",
      "55  ##3          30,138\n",
      "56  ##rh          4,077\n",
      "57  ##2          30,132\n",
      "58  ##ge            303\n",
      "59  ##2          30,132\n",
      "60  .               205\n",
      "61  [SEP]           103\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa6n2eQ9W05T"
   },
   "source": [
    "# **Running BERT on the text**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G-v2zmq89F4W",
    "outputId": "03692dbf-20f3-430d-9557-23b6ab3218ee",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(31090, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O2naiiBC9dw0"
   },
   "source": [
    "# Run the text through BERT, get the output and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor)\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on\n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case,\n",
    "    # becase we set `output_hidden_states = True`, the third item will be the\n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    last_hidden_state = outputs[0]\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "    # initial embeddings can be taken from 0th layer of hidden states\n",
    "    initial_embeddings = hidden_states[0]\n",
    "\n",
    "    # sum of all hidden states\n",
    "    sum_all_hidden_states = torch.stack(hidden_states).sum(0)\n",
    "\n",
    "    # sum of last four layer\n",
    "    sum_last_four_layers = torch.stack(hidden_states[-4:]).sum(0)\n",
    "\n",
    "    #concat last four layers\n",
    "    concat_last_four_layers = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1)"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bqi2trt1BfM8",
    "outputId": "c68c711a-a9a9-489d-fe75-869881c4c046",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    }
   },
   "source": [
    "sum_last_four_layers.size()"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 62, 768])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lgnQNZc89zxM",
    "outputId": "11a38872-9b2b-4675-ee9b-0394b0a8fc45",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    }
   },
   "source": [
    "concat_last_four_layers.size()"
   ],
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 62, 3072])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Output Layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 13   (initial embeddings + 12 BERT layers)\n",
      "Number of batches: 1\n",
      "Number of tokens: 62\n",
      "Number of hidden units: 768\n"
     ]
    }
   ],
   "source": [
    "print (\"Number of layers:\", len(hidden_states), \"  (initial embeddings + 12 BERT layers)\")\n",
    "layer_i = 0\n",
    "\n",
    "print (\"Number of batches:\", len(hidden_states[layer_i]))\n",
    "batch_i = 0\n",
    "\n",
    "print (\"Number of tokens:\", len(hidden_states[layer_i][batch_i]))\n",
    "token_i = 0\n",
    "\n",
    "print (\"Number of hidden units:\", len(hidden_states[layer_i][batch_i][token_i]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Type of hidden_states:  <class 'tuple'>\n",
      "Tensor shape for each layer:  torch.Size([1, 62, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([62, 13, 768])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `hidden_states` is a Python list.\n",
    "print('      Type of hidden_states: ', type(hidden_states))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', hidden_states[0].size())\n",
    "\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "# Remove batches dimension\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "# we swap dimension 0 (layers) and 1 (tokens)\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word vectors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Concatenate last 4 hidden layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 62 x 3072\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 3,072]\n",
    "token_vecs_cat = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Concatenate the vectors (that is, append them together) from the last\n",
    "    # four layers.\n",
    "    # Each layer vector is 768 values, so `cat_vec` is length 3,072.\n",
    "    cat_vec = torch.cat((token[-1], token[-2], token[-3], token[-4]), dim=0)\n",
    "\n",
    "    # Use `cat_vec` to represent `token`.\n",
    "    token_vecs_cat.append(cat_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_cat), len(token_vecs_cat[0])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sum last 4 hidden layers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 62 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis meaning\n",
    "\n",
    "We are trying to check"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   [CLS]           102\n",
      "1   we              185\n",
      "2   are             220\n",
      "3   studying      8,467\n",
      "4   the             111\n",
      "5   material      1,440\n",
      "6   la            1,665\n",
      "7   ##3          30,138\n",
      "8   ##a          30,110\n",
      "9   ##2          30,132\n",
      "10  ##ge            303\n",
      "11  ##2          30,132\n",
      "12  (               145\n",
      "13  a               106\n",
      "14  =               275\n",
      "15  ir            1,622\n",
      "16  ,               422\n",
      "17  rh            3,645\n",
      "18  )               546\n",
      "19  .               205\n",
      "20  [SEP]           103\n",
      "21  the             111\n",
      "22  critical      2,616\n",
      "23  temperature   1,633\n",
      "24  t               105\n",
      "25  c               115\n",
      "26  =               275\n",
      "27  4               286\n",
      "28  .               205\n",
      "29  7               450\n",
      "30  k               231\n",
      "31  discovered    8,847\n",
      "32  for             168\n",
      "33  la            1,665\n",
      "34  ##3          30,138\n",
      "35  ##ir            211\n",
      "36  ##2          30,132\n",
      "37  ##ge            303\n",
      "38  ##2          30,132\n",
      "39  in              121\n",
      "40  this            238\n",
      "41  work            697\n",
      "42  is              165\n",
      "43  by              214\n",
      "44  about         1,011\n",
      "45  1               158\n",
      "46  .               205\n",
      "47  2               170\n",
      "48  k               231\n",
      "49  higher        1,001\n",
      "50  than            506\n",
      "51  that            198\n",
      "52  found           797\n",
      "53  for             168\n",
      "54  la            1,665\n",
      "55  ##3          30,138\n",
      "56  ##rh          4,077\n",
      "57  ##2          30,132\n",
      "58  ##ge            303\n",
      "59  ##2          30,132\n",
      "60  .               205\n",
      "61  [SEP]           103\n"
     ]
    }
   ],
   "source": [
    "for idx, tup in enumerate(zip(tokenized_text, indexed_tokens)):\n",
    "    print('{:<3} {:<12} {:>6,}'.format(idx, tup[0], tup[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 vector values for each instance of \"materials\".\n",
      "\n",
      "la (pos 6) tensor([ 8.7684, -0.8787,  2.3963,  6.9049, -1.5473])\n",
      "la (pos 33) tensor([ 2.8659, -2.0691,  1.8861, 10.5369,  0.8178])\n",
      "la (pos 54) tensor([ 4.5353,  0.0495,  2.9821, 10.7882, -2.3673])\n"
     ]
    }
   ],
   "source": [
    "print('First 5 vector values for each instance of \"materials\".')\n",
    "print('')\n",
    "print(\"la (pos 6)\", str(token_vecs_sum[6][:5]))\n",
    "print(\"la (pos 33)\", str(token_vecs_sum[33][:5]))\n",
    "print(\"la (pos 54)\", str(token_vecs_sum[54][:5]))\n",
    "\n",
    "# print(\"ge  \", str(token_vecs_sum[10][:5]))\n",
    "# print(\"ir   \", str(token_vecs_sum[15][:5]))\n",
    "# print(\"la   \", str(token_vecs_sum[53][:5]))\n",
    "# print(\"rh   \", str(token_vecs_sum[55][:5]))\n",
    "# print(\"ge   \", str(token_vecs_sum[57][:5]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for la (pos 6) and la (pos 33) meanings:  0.80\n",
      "Vector similarity for  la (pos 6) and la (pos 54) meanings:  0.84\n",
      "Vector similarity for la (pos 33) and la (pos 54) meanings:  0.88\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Calculate the cosine similarity between the word bank\n",
    "# in \"bank robber\" vs \"river bank\" (different meanings).\n",
    "diff = 1 - cosine(token_vecs_sum[6], token_vecs_sum[33])\n",
    "print('Vector similarity for la (pos 6) and la (pos 33) meanings:  %.2f' % diff)\n",
    "\n",
    "diff = 1 - cosine(token_vecs_sum[6], token_vecs_sum[54])\n",
    "print('Vector similarity for  la (pos 6) and la (pos 54) meanings:  %.2f' % diff)\n",
    "\n",
    "diff = 1 - cosine(token_vecs_sum[33], token_vecs_sum[54])\n",
    "print('Vector similarity for la (pos 33) and la (pos 54) meanings:  %.2f' % diff)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}