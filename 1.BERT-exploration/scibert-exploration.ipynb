{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "BERT Word Embeddings Deep Dive.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcm5Iri7qE2q",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Installing the necessary libararies**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N7iUWdmx8FNu",
    "outputId": "9285377d-f224-417d-aeab-84246a2cf895",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install sklearn\n",
    "!pip install pytorch_transformers\n",
    "!pip install transformers\n",
    "!pip install bertviz"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: numpy in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (1.21.5)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torch in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (1.10.2)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from torch) (4.0.1)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sklearn in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sklearn) (1.0.2)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.1.0)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.7.3)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from scikit-learn->sklearn) (3.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.21.5)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: pytorch_transformers in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (1.2.0)\r\n",
      "Requirement already satisfied: regex in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (2022.1.18)\r\n",
      "Requirement already satisfied: numpy in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (1.21.5)\r\n",
      "Requirement already satisfied: requests in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (2.27.1)\r\n",
      "Requirement already satisfied: sentencepiece in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (0.1.96)\r\n",
      "Requirement already satisfied: torch>=1.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (1.10.2)\r\n",
      "Requirement already satisfied: boto3 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (1.20.49)\r\n",
      "Requirement already satisfied: tqdm in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (4.62.3)\r\n",
      "Requirement already satisfied: sacremoses in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (0.0.47)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from torch>=1.0.0->pytorch_transformers) (4.0.1)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from boto3->pytorch_transformers) (0.10.0)\r\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.49 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from boto3->pytorch_transformers) (1.23.49)\r\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from boto3->pytorch_transformers) (0.5.1)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.49->boto3->pytorch_transformers) (1.26.8)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.49->boto3->pytorch_transformers) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.49->boto3->pytorch_transformers) (1.16.0)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->pytorch_transformers) (2.0.11)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->pytorch_transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->pytorch_transformers) (3.3)\r\n",
      "Requirement already satisfied: click in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->pytorch_transformers) (8.0.3)\r\n",
      "Requirement already satisfied: joblib in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->pytorch_transformers) (1.1.0)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from click->sacremoses->pytorch_transformers) (4.10.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->pytorch_transformers) (3.7.0)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (4.16.2)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: filelock in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (3.4.2)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (0.11.4)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (0.4.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (2022.1.18)\r\n",
      "Requirement already satisfied: sacremoses in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (0.0.47)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (4.10.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (1.21.5)\r\n",
      "Requirement already satisfied: requests in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (2.27.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (4.62.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.7)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->transformers) (2.0.11)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->transformers) (3.3)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\r\n",
      "Requirement already satisfied: click in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\r\n",
      "Requirement already satisfied: joblib in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\r\n",
      "Requirement already satisfied: six in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: bertviz in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (1.3.0)\r\n",
      "Requirement already satisfied: regex in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from bertviz) (2022.1.18)\r\n",
      "Requirement already satisfied: torch>=1.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from bertviz) (1.10.2)\r\n",
      "Requirement already satisfied: boto3 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from bertviz) (1.20.49)\r\n",
      "Requirement already satisfied: tqdm in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from bertviz) (4.62.3)\r\n",
      "Requirement already satisfied: transformers>=2.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from bertviz) (4.16.2)\r\n",
      "Requirement already satisfied: sentencepiece in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from bertviz) (0.1.96)\r\n",
      "Requirement already satisfied: requests in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from bertviz) (2.27.1)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from torch>=1.0->bertviz) (4.0.1)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers>=2.0->bertviz) (0.11.4)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers>=2.0->bertviz) (4.10.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers>=2.0->bertviz) (1.21.5)\r\n",
      "Requirement already satisfied: filelock in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers>=2.0->bertviz) (3.4.2)\r\n",
      "Requirement already satisfied: sacremoses in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers>=2.0->bertviz) (0.0.47)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers>=2.0->bertviz) (6.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers>=2.0->bertviz) (21.3)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers>=2.0->bertviz) (0.4.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from packaging>=20.0->transformers>=2.0->bertviz) (3.0.7)\r\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.49 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from boto3->bertviz) (1.23.49)\r\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from boto3->bertviz) (0.5.1)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from boto3->bertviz) (0.10.0)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.49->boto3->bertviz) (2.8.2)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.49->boto3->bertviz) (1.26.8)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.49->boto3->bertviz) (1.16.0)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from importlib-metadata->transformers>=2.0->bertviz) (3.7.0)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->bertviz) (2.0.11)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->bertviz) (2021.10.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->bertviz) (3.3)\r\n",
      "Requirement already satisfied: joblib in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->transformers>=2.0->bertviz) (1.1.0)\r\n",
      "Requirement already satisfied: click in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->transformers>=2.0->bertviz) (8.0.3)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZovDAtnWQsg",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Loading the Pre-trained BERT model**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xKtXV_rd8cwa",
    "outputId": "a26426be-429b-4b2f-99c3-7263b372ba63",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/Users/lfoppiano/development/projects/embeddings/pre-trained-embeddings/scibert/scibert_scivocab_cased_hf\")\n",
    "model = AutoModel.from_pretrained(\"/Users/lfoppiano/development/projects/embeddings/pre-trained-embeddings/scibert/scibert_scivocab_cased_hf\",output_hidden_states=True)\n"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/lfoppiano/development/projects/embeddings/pre-trained-embeddings/scibert/scibert_scivocab_cased_hf were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGbXXZ9_WfFX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Input Formatting (Tokenization)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XzUlQMCA8RpQ",
    "outputId": "820a23f7-779d-42aa-cb23-7917fbf49ca4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text1 = \"We are studying the material La3A2Ge2 (A = Ir, Rh).\"\n",
    "text2 = \"The critical temperature T C = 4.7 K discovered for La3Ir2Ge2 in this work is by about 1.2 K higher than that found for La3Rh2Ge2.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text1 + \" [SEP] \" + text2 + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for idx, tup in enumerate(zip(tokenized_text, indexed_tokens)):\n",
    "    print('{:<3} {:<12} {:>6,}'.format(idx, tup[0], tup[1]))"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   [CLS]           101\n",
      "1   we              289\n",
      "2   are             231\n",
      "3   studying      9,542\n",
      "4   the             111\n",
      "5   material      1,804\n",
      "6   la            2,326\n",
      "7   ##3          30,141\n",
      "8   ##a          30,108\n",
      "9   ##2          30,130\n",
      "10  ##ge            314\n",
      "11  ##2          30,130\n",
      "12  (               143\n",
      "13  a               105\n",
      "14  =               281\n",
      "15  ir            2,519\n",
      "16  ,               430\n",
      "17  rh            8,651\n",
      "18  )               551\n",
      "19  .               211\n",
      "20  [SEP]           102\n",
      "21  the             111\n",
      "22  critical      2,771\n",
      "23  temperature   1,674\n",
      "24  t               104\n",
      "25  c               116\n",
      "26  =               281\n",
      "27  4               294\n",
      "28  .               211\n",
      "29  7               452\n",
      "30  k               324\n",
      "31  discovered    9,556\n",
      "32  for             173\n",
      "33  la            2,326\n",
      "34  ##3          30,141\n",
      "35  ##ir            220\n",
      "36  ##2          30,130\n",
      "37  ##ge            314\n",
      "38  ##2          30,130\n",
      "39  in              124\n",
      "40  this            306\n",
      "41  work            743\n",
      "42  is              163\n",
      "43  by              224\n",
      "44  about         1,036\n",
      "45  1               155\n",
      "46  .               211\n",
      "47  2               166\n",
      "48  k               324\n",
      "49  higher        1,027\n",
      "50  than            515\n",
      "51  that            202\n",
      "52  found           858\n",
      "53  for             173\n",
      "54  la            2,326\n",
      "55  ##3          30,141\n",
      "56  ##rh         25,315\n",
      "57  ##2          30,130\n",
      "58  ##ge            314\n",
      "59  ##2          30,130\n",
      "60  .               211\n",
      "61  [SEP]           102\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa6n2eQ9W05T",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# **Running BERT on the text**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G-v2zmq89F4W",
    "outputId": "03692dbf-20f3-430d-9557-23b6ab3218ee",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "BertModel(\n  (embeddings): BertEmbeddings(\n    (word_embeddings): Embedding(31116, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (token_type_embeddings): Embedding(2, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): BertEncoder(\n    (layer): ModuleList(\n      (0): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): BertLayer(\n        (attention): BertAttention(\n          (self): BertSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): BertSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): BertIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): BertOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): BertPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O2naiiBC9dw0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "# Run the text through BERT, get the output and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor)\n",
    "\n",
    "    # can use last hidden state as word embeddings\n",
    "    last_hidden_state = outputs[0]\n",
    "    word_embed_1 = last_hidden_state\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "    # initial embeddings can be taken from 0th layer of hidden states\n",
    "    word_embed_2 = hidden_states[0]\n",
    "\n",
    "    # sum of all hidden states\n",
    "    word_embed_3 = torch.stack(hidden_states).sum(0)\n",
    "\n",
    "    # sum of second to last layer\n",
    "    word_embed_4 = torch.stack(hidden_states[2:]).sum(0) \n",
    "\n",
    "    # sum of last four layer\n",
    "    word_embed_5 = torch.stack(hidden_states[-4:]).sum(0) \n",
    "\n",
    "    #concat last four layers\n",
    "    word_embed_6 = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1)\n",
    "\n",
    "\n"
   ],
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bqi2trt1BfM8",
    "outputId": "c68c711a-a9a9-489d-fe75-869881c4c046",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "word_embed_5.size()"
   ],
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 62, 768])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lgnQNZc89zxM",
    "outputId": "11a38872-9b2b-4675-ee9b-0394b0a8fc45",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "word_embed_6.size()"
   ],
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 62, 3072])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Type of hidden_states:  <class 'tuple'>\n",
      "Tensor shape for each layer:  torch.Size([1, 62, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([62, 13, 768])"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `hidden_states` is a Python list.\n",
    "print('      Type of hidden_states: ', type(hidden_states))\n",
    "\n",
    "# Each layer in the list is a torch tensor.\n",
    "print('Tensor shape for each layer: ', hidden_states[0].size())\n",
    "\n",
    "token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "# Remove batches dimension\n",
    "token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "# we swap dimension 0 (layers) and 1 (tokens)\n",
    "token_embeddings = token_embeddings.permute(1,0,2)\n",
    "\n",
    "token_embeddings.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape is: 62 x 768\n"
     ]
    }
   ],
   "source": [
    "# Stores the token vectors, with shape [22 x 768]\n",
    "token_vecs_sum = []\n",
    "\n",
    "# `token_embeddings` is a [22 x 12 x 768] tensor.\n",
    "\n",
    "# For each token in the sentence...\n",
    "for token in token_embeddings:\n",
    "\n",
    "    # `token` is a [12 x 768] tensor\n",
    "\n",
    "    # Sum the vectors from the last four layers.\n",
    "    sum_vec = torch.sum(token[-4:], dim=0)\n",
    "\n",
    "    # Use `sum_vec` to represent `token`.\n",
    "    token_vecs_sum.append(sum_vec)\n",
    "\n",
    "print ('Shape is: %d x %d' % (len(token_vecs_sum), len(token_vecs_sum[0])))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Analysis meaning\n",
    "\n",
    "We are trying to check"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   [CLS]           101\n",
      "1   we              289\n",
      "2   are             231\n",
      "3   studying      9,542\n",
      "4   the             111\n",
      "5   material      1,804\n",
      "6   la            2,326\n",
      "7   ##3          30,141\n",
      "8   ##a          30,108\n",
      "9   ##2          30,130\n",
      "10  ##ge            314\n",
      "11  ##2          30,130\n",
      "12  (               143\n",
      "13  a               105\n",
      "14  =               281\n",
      "15  ir            2,519\n",
      "16  ,               430\n",
      "17  rh            8,651\n",
      "18  )               551\n",
      "19  .               211\n",
      "20  [SEP]           102\n",
      "21  the             111\n",
      "22  critical      2,771\n",
      "23  temperature   1,674\n",
      "24  t               104\n",
      "25  c               116\n",
      "26  =               281\n",
      "27  4               294\n",
      "28  .               211\n",
      "29  7               452\n",
      "30  k               324\n",
      "31  discovered    9,556\n",
      "32  for             173\n",
      "33  la            2,326\n",
      "34  ##3          30,141\n",
      "35  ##ir            220\n",
      "36  ##2          30,130\n",
      "37  ##ge            314\n",
      "38  ##2          30,130\n",
      "39  in              124\n",
      "40  this            306\n",
      "41  work            743\n",
      "42  is              163\n",
      "43  by              224\n",
      "44  about         1,036\n",
      "45  1               155\n",
      "46  .               211\n",
      "47  2               166\n",
      "48  k               324\n",
      "49  higher        1,027\n",
      "50  than            515\n",
      "51  that            202\n",
      "52  found           858\n",
      "53  for             173\n",
      "54  la            2,326\n",
      "55  ##3          30,141\n",
      "56  ##rh         25,315\n",
      "57  ##2          30,130\n",
      "58  ##ge            314\n",
      "59  ##2          30,130\n",
      "60  .               211\n",
      "61  [SEP]           102\n"
     ]
    }
   ],
   "source": [
    "for idx, tup in enumerate(zip(tokenized_text, indexed_tokens)):\n",
    "    print('{:<3} {:<12} {:>6,}'.format(idx, tup[0], tup[1]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "la (pos 6) tensor([ 1.5464,  0.0417,  2.5421,  0.1350, -3.2658])\n",
      "la (pos 33) tensor([ 1.8206,  4.5411, -0.8660, -0.8465, -2.9136])\n",
      "la (pos 54) tensor([ 0.5909,  4.3853,  1.2380, -2.0107, -0.4635])\n"
     ]
    }
   ],
   "source": [
    "print('')\n",
    "print(\"la (pos 6)\", str(token_vecs_sum[6][:5]))\n",
    "print(\"la (pos 33)\", str(token_vecs_sum[33][:5]))\n",
    "print(\"la (pos 54)\", str(token_vecs_sum[54][:5]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector similarity for la (pos 6) and la (pos 33) meanings:  0.81\n",
      "Vector similarity for  (pos 6) and la (pos 53) meanings:  0.76\n",
      "Vector similarity for la (pos 33) and la (pos 53) meanings:  0.87\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "diff = 1 - cosine(token_vecs_sum[6], token_vecs_sum[33])\n",
    "print('Vector similarity for la (pos 6) and la (pos 33) meanings:  %.2f' % diff)\n",
    "\n",
    "diff = 1 - cosine(token_vecs_sum[6], token_vecs_sum[54])\n",
    "print('Vector similarity for  (pos 6) and la (pos 53) meanings:  %.2f' % diff)\n",
    "\n",
    "diff = 1 - cosine(token_vecs_sum[33], token_vecs_sum[54])\n",
    "print('Vector similarity for la (pos 33) and la (pos 53) meanings:  %.2f' % diff)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Display attention"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The attention tensor does not have the correct number of dimensions. Make sure you set output_attentions=True when initializing your model.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/mk/scd8428n18jfgh3jdthbvpz00000gn/T/ipykernel_32105/3460204963.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mtokens\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtokenizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mconvert_ids_to_tokens\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtokens_tensor\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# Convert input ids to token strings\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 6\u001B[0;31m \u001B[0mhead_view\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattention\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtokens\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages/bertviz/head_view.py\u001B[0m in \u001B[0;36mhead_view\u001B[0;34m(attention, tokens, sentence_b_start, prettify_tokens, layer, heads, encoder_attention, decoder_attention, cross_attention, encoder_tokens, decoder_tokens, include_layers)\u001B[0m\n\u001B[1;32m     57\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0minclude_layers\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m             \u001B[0minclude_layers\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlist\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnum_layers\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattention\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 59\u001B[0;31m         \u001B[0mattention\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mformat_attention\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mattention\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minclude_layers\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     60\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0msentence_b_start\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     61\u001B[0m             attn_data.append(\n",
      "\u001B[0;32m~/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages/bertviz/util.py\u001B[0m in \u001B[0;36mformat_attention\u001B[0;34m(attention, layers, heads)\u001B[0m\n\u001B[1;32m      9\u001B[0m         \u001B[0;31m# 1 x num_heads x seq_len x seq_len\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayer_attention\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m!=\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m             raise ValueError(\"The attention tensor does not have the correct number of dimensions. Make sure you set \"\n\u001B[0m\u001B[1;32m     12\u001B[0m                              \"output_attentions=True when initializing your model.\")\n\u001B[1;32m     13\u001B[0m         \u001B[0mlayer_attention\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlayer_attention\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: The attention tensor does not have the correct number of dimensions. Make sure you set output_attentions=True when initializing your model."
     ]
    }
   ],
   "source": [
    "from bertviz import model_view, head_view\n",
    "\n",
    "attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor[0])  # Convert input ids to token strings\n",
    "head_view(attention, tokens)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}