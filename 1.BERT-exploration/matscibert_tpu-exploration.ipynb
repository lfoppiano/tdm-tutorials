{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "BERT Word Embeddings Deep Dive.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcm5Iri7qE2q"
   },
   "source": [
    "# **Installing the necessary libararies**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N7iUWdmx8FNu",
    "outputId": "9285377d-f224-417d-aeab-84246a2cf895",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install sklearn\n",
    "!pip install pytorch_transformers\n",
    "!pip install transformers"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (1.21.5)\r\n",
      "Requirement already satisfied: torch in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (1.10.2)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from torch) (4.0.1)\r\n",
      "Requirement already satisfied: sklearn in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sklearn) (1.0.2)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.7.3)\r\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.21.5)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from scikit-learn->sklearn) (3.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from scikit-learn->sklearn) (1.1.0)\r\n",
      "Requirement already satisfied: pytorch_transformers in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (1.2.0)\r\n",
      "Requirement already satisfied: numpy in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (1.21.5)\r\n",
      "Requirement already satisfied: tqdm in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (4.62.3)\r\n",
      "Requirement already satisfied: requests in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (2.27.1)\r\n",
      "Requirement already satisfied: sentencepiece in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (0.1.96)\r\n",
      "Requirement already satisfied: sacremoses in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (0.0.47)\r\n",
      "Requirement already satisfied: boto3 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (1.20.49)\r\n",
      "Requirement already satisfied: regex in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (2022.1.18)\r\n",
      "Requirement already satisfied: torch>=1.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from pytorch_transformers) (1.10.2)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from torch>=1.0.0->pytorch_transformers) (4.0.1)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from boto3->pytorch_transformers) (0.10.0)\r\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from boto3->pytorch_transformers) (0.5.1)\r\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.49 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from boto3->pytorch_transformers) (1.23.49)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.49->boto3->pytorch_transformers) (2.8.2)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from botocore<1.24.0,>=1.23.49->boto3->pytorch_transformers) (1.26.8)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.49->boto3->pytorch_transformers) (1.16.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->pytorch_transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->pytorch_transformers) (2.0.11)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->pytorch_transformers) (3.3)\r\n",
      "Requirement already satisfied: joblib in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->pytorch_transformers) (1.1.0)\r\n",
      "Requirement already satisfied: click in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->pytorch_transformers) (8.0.3)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from click->sacremoses->pytorch_transformers) (4.10.1)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from importlib-metadata->click->sacremoses->pytorch_transformers) (3.7.0)\r\n",
      "Requirement already satisfied: transformers in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (4.16.2)\r\n",
      "Requirement already satisfied: importlib-metadata in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (4.10.1)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (2022.1.18)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (0.11.4)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (0.4.0)\r\n",
      "Requirement already satisfied: requests in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (2.27.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (1.21.5)\r\n",
      "Requirement already satisfied: sacremoses in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (0.0.47)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (4.62.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: filelock in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (3.4.2)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.0.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.7)\r\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.7.0)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->transformers) (2.0.11)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->transformers) (1.26.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->transformers) (3.3)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from requests->transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: six in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "Requirement already satisfied: joblib in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->transformers) (1.1.0)\r\n",
      "Requirement already satisfied: click in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.7/site-packages (from sacremoses->transformers) (8.0.3)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MZovDAtnWQsg"
   },
   "source": [
    "# **Loading the Pre-trained BERT model**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xKtXV_rd8cwa",
    "outputId": "a26426be-429b-4b2f-99c3-7263b372ba63",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    }
   },
   "source": [
    "from transformers import AutoTokenizer, AutoModel, BertTokenizer\n",
    "\n",
    "## Load pretrained model/tokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# model = BertModel.from_pretrained('bert-base-uncased',output_hidden_states=True)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"/Users/lfoppiano/development/projects/embeddings/pre-trained-embeddings/matscibert_tpu/scivocab.myvocab.txt\")\n",
    "# model = AutoModel.from_pretrained(\"/Users/lfoppiano/development/projects/embeddings/pre-trained-embeddings/matscibert_tpu/\",output_hidden_states=True)"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGbXXZ9_WfFX"
   },
   "source": [
    "# **Input Formatting (Tokenization)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XzUlQMCA8RpQ",
    "outputId": "820a23f7-779d-42aa-cb23-7917fbf49ca4",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    }
   },
   "source": [
    "# Define a new example sentence with multiple meanings of the word \"bank\"\n",
    "text = \"We are studying the material La 3 A 2 Ge 2 (A = Ir, Rh). The critical temperature T C = 4.7 K discovered for La 3 Ir 2 Ge 2 in this work is by about 1.2 K higher than that found for La 3 Rh 2 Ge 2.\"\n",
    "\n",
    "# Add the special tokens.\n",
    "marked_text = \"[CLS] \" + text + \" [SEP]\"\n",
    "\n",
    "# Split the sentence into tokens.\n",
    "tokenized_text = tokenizer.tokenize(marked_text)\n",
    "\n",
    "# Map the token strings to their vocabulary indeces.\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Display the words with their indeces.\n",
    "for tup in zip(tokenized_text, indexed_tokens):\n",
    "    print('{:<12} {:>6,}'.format(tup[0], tup[1]))"
   ],
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                63\n",
      "C                39\n",
      "LS            3,148\n",
      "]                65\n",
      "ĠWe             784\n",
      "Ġare            325\n",
      "Ġstudying     7,866\n",
      "Ġthe            216\n",
      "Ġmaterial       715\n",
      "ĠLa           3,575\n",
      "Ġ3              370\n",
      "ĠA              302\n",
      "Ġ2              322\n",
      "ĠGe           3,273\n",
      "Ġ2              322\n",
      "Ġ(              249\n",
      "A                37\n",
      "Ġ=              557\n",
      "ĠIr           5,302\n",
      ",                16\n",
      "ĠRh           4,982\n",
      ").              356\n",
      "ĠThe            275\n",
      "Ġcritical     2,328\n",
      "Ġtemperature    505\n",
      "ĠT              255\n",
      "ĠC              303\n",
      "Ġ=              557\n",
      "Ġ4              436\n",
      ".                18\n",
      "7                27\n",
      "ĠK              680\n",
      "Ġdiscovered   9,238\n",
      "Ġfor            271\n",
      "ĠLa           3,575\n",
      "Ġ3              370\n",
      "ĠIr           5,302\n",
      "Ġ2              322\n",
      "ĠGe           3,273\n",
      "Ġ2              322\n",
      "Ġin             234\n",
      "Ġthis           445\n",
      "Ġwork         1,020\n",
      "Ġis             268\n",
      "Ġby             316\n",
      "Ġabout        1,276\n",
      "Ġ1              278\n",
      ".                18\n",
      "2                22\n",
      "ĠK              680\n",
      "Ġhigher         866\n",
      "Ġthan           607\n",
      "Ġthat           317\n",
      "Ġfound          950\n",
      "Ġfor            271\n",
      "ĠLa           3,575\n",
      "Ġ3              370\n",
      "ĠRh           4,982\n",
      "Ġ2              322\n",
      "ĠGe           3,273\n",
      "Ġ2              322\n",
      ".                18\n",
      "Ġ[              417\n",
      "S                55\n",
      "EP            5,105\n",
      "]                65\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa6n2eQ9W05T"
   },
   "source": [
    "# **Running BERT on the text**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "G-v2zmq89F4W",
    "outputId": "03692dbf-20f3-430d-9557-23b6ab3218ee",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    }
   },
   "source": [
    "import torch\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "# Put the model in \"evaluation\" mode, meaning feed-forward operation.\n",
    "model.eval()\n"
   ],
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "RobertaModel(\n  (embeddings): RobertaEmbeddings(\n    (word_embeddings): Embedding(32768, 768, padding_idx=1)\n    (position_embeddings): Embedding(514, 768, padding_idx=1)\n    (token_type_embeddings): Embedding(1, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (encoder): RobertaEncoder(\n    (layer): ModuleList(\n      (0): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (1): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (2): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (3): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (4): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (5): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (6): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (7): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (8): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (9): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (10): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n      (11): RobertaLayer(\n        (attention): RobertaAttention(\n          (self): RobertaSelfAttention(\n            (query): Linear(in_features=768, out_features=768, bias=True)\n            (key): Linear(in_features=768, out_features=768, bias=True)\n            (value): Linear(in_features=768, out_features=768, bias=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (output): RobertaSelfOutput(\n            (dense): Linear(in_features=768, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (intermediate): RobertaIntermediate(\n          (dense): Linear(in_features=768, out_features=3072, bias=True)\n        )\n        (output): RobertaOutput(\n          (dense): Linear(in_features=3072, out_features=768, bias=True)\n          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n  )\n  (pooler): RobertaPooler(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): Tanh()\n  )\n)"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "O2naiiBC9dw0"
   },
   "source": [
    "# Run the text through BERT, get the output and collect all of the hidden states produced\n",
    "# from all 12 layers. \n",
    "with torch.no_grad():\n",
    "\n",
    "    outputs = model(tokens_tensor)\n",
    "\n",
    "    # can use last hidden state as word embeddings\n",
    "    last_hidden_state = outputs[0]\n",
    "    word_embed_1 = last_hidden_state\n",
    "\n",
    "    # Evaluating the model will return a different number of objects based on \n",
    "    # how it's  configured in the `from_pretrained` call earlier. In this case, \n",
    "    # becase we set `output_hidden_states = True`, the third item will be the \n",
    "    # hidden states from all layers. See the documentation for more details:\n",
    "    # https://huggingface.co/transformers/model_doc/bert.html#bertmodel\n",
    "    hidden_states = outputs[2]\n",
    "\n",
    "    # initial embeddings can be taken from 0th layer of hidden states\n",
    "    word_embed_2 = hidden_states[0]\n",
    "\n",
    "    # sum of all hidden states\n",
    "    word_embed_3 = torch.stack(hidden_states).sum(0)\n",
    "\n",
    "    # sum of second to last layer\n",
    "    word_embed_4 = torch.stack(hidden_states[2:]).sum(0) \n",
    "\n",
    "    # sum of last four layer\n",
    "    word_embed_5 = torch.stack(hidden_states[-4:]).sum(0) \n",
    "\n",
    "    #concat last four layers\n",
    "    word_embed_6 = torch.cat([hidden_states[i] for i in [-1,-2,-3,-4]], dim=-1)\n",
    "\n",
    "\n"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Bqi2trt1BfM8",
    "outputId": "c68c711a-a9a9-489d-fe75-869881c4c046",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    }
   },
   "source": [
    "word_embed_5.size()"
   ],
   "execution_count": 29,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 19, 768])"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lgnQNZc89zxM",
    "outputId": "11a38872-9b2b-4675-ee9b-0394b0a8fc45",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    }
   },
   "source": [
    "word_embed_6.size()"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 19, 3072])"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}