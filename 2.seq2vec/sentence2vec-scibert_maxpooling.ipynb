{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "BERT Word Embeddings Deep Dive.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hcm5Iri7qE2q",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Installing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N7iUWdmx8FNu",
    "outputId": "9285377d-f224-417d-aeab-84246a2cf895",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "!pip install numpy\n",
    "!pip install torch\n",
    "!pip install sklearn\n",
    "!pip install pytorch_transformers\n",
    "!pip install transformers"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: numpy in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (1.22.3)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: torch in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (1.11.0)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from torch) (4.1.1)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: sklearn in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (0.0)\r\n",
      "Requirement already satisfied: scikit-learn in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from sklearn) (1.0.2)\r\n",
      "Requirement already satisfied: scipy>=1.1.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.8.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from scikit-learn->sklearn) (3.1.0)\r\n",
      "Requirement already satisfied: numpy>=1.14.6 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.22.3)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from scikit-learn->sklearn) (1.1.0)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: pytorch_transformers in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (1.2.0)\r\n",
      "Requirement already satisfied: sentencepiece in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from pytorch_transformers) (0.1.96)\r\n",
      "Requirement already satisfied: regex in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from pytorch_transformers) (2022.3.15)\r\n",
      "Requirement already satisfied: requests in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from pytorch_transformers) (2.27.1)\r\n",
      "Requirement already satisfied: tqdm in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from pytorch_transformers) (4.64.0)\r\n",
      "Requirement already satisfied: torch>=1.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from pytorch_transformers) (1.11.0)\r\n",
      "Requirement already satisfied: sacremoses in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from pytorch_transformers) (0.0.49)\r\n",
      "Requirement already satisfied: boto3 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from pytorch_transformers) (1.21.37)\r\n",
      "Requirement already satisfied: numpy in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from pytorch_transformers) (1.22.3)\r\n",
      "Requirement already satisfied: typing-extensions in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from torch>=1.0.0->pytorch_transformers) (4.1.1)\r\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from boto3->pytorch_transformers) (1.0.0)\r\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from boto3->pytorch_transformers) (0.5.2)\r\n",
      "Requirement already satisfied: botocore<1.25.0,>=1.24.37 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from boto3->pytorch_transformers) (1.24.37)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from requests->pytorch_transformers) (1.26.9)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from requests->pytorch_transformers) (2.0.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from requests->pytorch_transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from requests->pytorch_transformers) (3.3)\r\n",
      "Requirement already satisfied: six in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from sacremoses->pytorch_transformers) (1.16.0)\r\n",
      "Requirement already satisfied: joblib in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from sacremoses->pytorch_transformers) (1.1.0)\r\n",
      "Requirement already satisfied: click in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from sacremoses->pytorch_transformers) (8.1.2)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from botocore<1.25.0,>=1.24.37->boto3->pytorch_transformers) (2.8.2)\r\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: transformers in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (4.18.0)\r\n",
      "Requirement already satisfied: requests in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from transformers) (2.27.1)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from transformers) (1.22.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: sacremoses in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from transformers) (0.0.49)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from transformers) (4.64.0)\r\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from transformers) (2022.3.15)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from transformers) (0.5.1)\r\n",
      "Requirement already satisfied: filelock in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from transformers) (3.6.0)\r\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from transformers) (0.11.6)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from transformers) (21.3)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from packaging>=20.0->transformers) (3.0.8)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from requests->transformers) (3.3)\r\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from requests->transformers) (2.0.12)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\r\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from requests->transformers) (1.26.9)\r\n",
      "Requirement already satisfied: click in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from sacremoses->transformers) (8.1.2)\r\n",
      "Requirement already satisfied: six in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from sacremoses->transformers) (1.16.0)\r\n",
      "Requirement already satisfied: joblib in /Users/lfoppiano/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages (from sacremoses->transformers) (1.1.0)\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# SciBERT"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /Users/lfoppiano/development/projects/embeddings/pre-trained-embeddings/scibert/scibert_scivocab_cased_hf were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"/Users/lfoppiano/development/projects/embeddings/pre-trained-embeddings/scibert/scibert_scivocab_cased_hf\")\n",
    "model = AutoModel.from_pretrained(\"/Users/lfoppiano/development/projects/embeddings/pre-trained-embeddings/scibert/scibert_scivocab_cased_hf\",output_hidden_states=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vector calculation\n",
    "\n",
    "Borrowed at https://github.com/stockmarkteam/bert-book/blob/master/Chapter10.ipynb\n",
    "thanks to Oka-san\n",
    "\n",
    "This method averages the last hidden state values, using the attention mask for coefficients\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./resources/supermat_classification.1.tsv\n",
      "./resources/supermat_classification.0.tsv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "\n",
    "def max_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    token_embeddings[input_mask_expanded == 0] = -1e9  # Set padding tokens to large negative value\n",
    "    return torch.max(token_embeddings, 1)[0]\n",
    "\n",
    "max_length = 256\n",
    "sentence_vectors = []\n",
    "classifications = []\n",
    "attention_masks = []\n",
    "for file in glob.glob(f'./resources/supermat_classification.?.tsv'):\n",
    "    print(file)\n",
    "    lines = open(file).read().splitlines()\n",
    "    for line in lines:\n",
    "        split = line.split(\"\\t\")\n",
    "        if len(split) < 2:\n",
    "            print(\"skip \", line)\n",
    "            continue\n",
    "\n",
    "        text = split[1]\n",
    "        classification = split[2]\n",
    "\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        encoding = { k: v for k, v in encoding.items() }\n",
    "        attention_mask = encoding['attention_mask']\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoding)\n",
    "            # last_hidden_state = output.last_hidden_state\n",
    "            # cross_attention = output.cross_attentions\n",
    "            # averaged_hidden_state = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(1) / attention_mask.sum(1, keepdim=True)\n",
    "\n",
    "        sentence_vectors = max_pooling(output, attention_mask)\n",
    "        classifications.append(classification)\n",
    "\n",
    "\n",
    "sentence_vectors = np.vstack(sentence_vectors)\n",
    "labels = np.array(classifications)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dimension reduction\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components=2 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[0;32mIn [10]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdecomposition\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PCA\n\u001B[0;32m----> 3\u001B[0m sentence_vectors_pca \u001B[38;5;241m=\u001B[39m \u001B[43mPCA\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_components\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43msentence_vectors\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:407\u001B[0m, in \u001B[0;36mPCA.fit_transform\u001B[0;34m(self, X, y)\u001B[0m\n\u001B[1;32m    385\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit_transform\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    386\u001B[0m     \u001B[38;5;124;03m\"\"\"Fit the model with X and apply the dimensionality reduction on X.\u001B[39;00m\n\u001B[1;32m    387\u001B[0m \n\u001B[1;32m    388\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    405\u001B[0m \u001B[38;5;124;03m    C-ordered array, use 'np.ascontiguousarray'.\u001B[39;00m\n\u001B[1;32m    406\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 407\u001B[0m     U, S, Vt \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    408\u001B[0m     U \u001B[38;5;241m=\u001B[39m U[:, : \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_components_]\n\u001B[1;32m    410\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwhiten:\n\u001B[1;32m    411\u001B[0m         \u001B[38;5;66;03m# X_new = X * V / S * sqrt(n_samples) = U * sqrt(n_samples)\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:457\u001B[0m, in \u001B[0;36mPCA._fit\u001B[0;34m(self, X)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;66;03m# Call different fits for either full or truncated SVD\u001B[39;00m\n\u001B[1;32m    456\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_svd_solver \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfull\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 457\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_full\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_components\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    458\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_svd_solver \u001B[38;5;129;01min\u001B[39;00m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marpack\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrandomized\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[1;32m    459\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_truncated(X, n_components, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fit_svd_solver)\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/bert_tutorial/lib/python3.8/site-packages/sklearn/decomposition/_pca.py:475\u001B[0m, in \u001B[0;36mPCA._fit_full\u001B[0;34m(self, X, n_components)\u001B[0m\n\u001B[1;32m    471\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    472\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_components=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmle\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is only supported if n_samples >= n_features\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    473\u001B[0m         )\n\u001B[1;32m    474\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;241m0\u001B[39m \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m n_components \u001B[38;5;241m<\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mmin\u001B[39m(n_samples, n_features):\n\u001B[0;32m--> 475\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    476\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mn_components=\u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m must be between 0 and \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    477\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmin(n_samples, n_features)=\u001B[39m\u001B[38;5;132;01m%r\u001B[39;00m\u001B[38;5;124m with \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    478\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msvd_solver=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfull\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (n_components, \u001B[38;5;28mmin\u001B[39m(n_samples, n_features))\n\u001B[1;32m    479\u001B[0m     )\n\u001B[1;32m    480\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m n_components \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    481\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(n_components, numbers\u001B[38;5;241m.\u001B[39mIntegral):\n",
      "\u001B[0;31mValueError\u001B[0m: n_components=2 must be between 0 and min(n_samples, n_features)=1 with svd_solver='full'"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sentence_vectors_pca = PCA(n_components=2).fit_transform(sentence_vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# plot"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "for label in range(2):\n",
    "    plt.subplot(2,2,label+1)\n",
    "    # index = [True if l == label else False for l in labels]\n",
    "    index = str(label) == labels\n",
    "    plt.plot(\n",
    "        sentence_vectors_pca[:,0],\n",
    "        sentence_vectors_pca[:,1],\n",
    "        'o',\n",
    "        markersize=1,\n",
    "        color=[0.7, 0.7, 0.7]\n",
    "    )\n",
    "    plt.plot(\n",
    "        sentence_vectors_pca[index,0],\n",
    "        sentence_vectors_pca[index,1],\n",
    "        'o',\n",
    "        markersize=2,\n",
    "        color='k'\n",
    "    )\n",
    "    plt.title(label)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "sentence_vectors_tsne = TSNE(n_components=2).fit_transform(sentence_vectors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "for label in range(2):\n",
    "    plt.subplot(2,2,label+1)\n",
    "    # index = [True if l == label else False for l in labels]\n",
    "    index = str(label) == labels\n",
    "    plt.plot(\n",
    "        sentence_vectors_tsne[:,0],\n",
    "        sentence_vectors_tsne[:,1],\n",
    "        'o',\n",
    "        markersize=1,\n",
    "        color=[0.7, 0.7, 0.7]\n",
    "    )\n",
    "    plt.plot(\n",
    "        sentence_vectors_tsne[index,0],\n",
    "        sentence_vectors_tsne[index,1],\n",
    "        'o',\n",
    "        markersize=2,\n",
    "        color='k'\n",
    "    )\n",
    "    plt.title(labels[label])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}